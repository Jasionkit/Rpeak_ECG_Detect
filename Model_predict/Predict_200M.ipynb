{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "    \n",
    "1. Seperate the whole record into multi batches; Feed batch to trained model; Get probability of each point\n",
    "\n",
    "\n",
    "2. Apply median filter to model output and binarize output using threshold = 0.5\n",
    "\n",
    "\n",
    "3. Refine prediction:\n",
    "\n",
    "    1) Filter out predicted QRS periods with width <100ms\n",
    "    \n",
    "    2) For two too closed QRS periods(<0.2s), discard discard the narrower one\n",
    "    \n",
    "    3) For two too distant QRS periods(>1,5s), re-search the gap for missing QRS using lower threshold = 0.4\n",
    "    \n",
    "    4) Filter out predicted QRS periods with width <100ms again\n",
    "\n",
    "\n",
    "4. Process true annotation and predicted R peak position; Calculate recall and precision (tolerance = 8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, h5py, time\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "from Stationary_transform import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_value(y_true, y_pred): # custom f1 score metric\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(ecg,model_path): \n",
    "    batch_size = int(LEN/BATCH_NUM)\n",
    "    prob = np.empty(LEN, 'float16')\n",
    "    data_X = np.empty((batch_size,4098),'float16')\n",
    "    \n",
    "    for i in range(BATCH_NUM):\n",
    "        #print('Batch '+ str(i))\n",
    "        start = time.time()\n",
    "        \n",
    "        seq = ecg[i*batch_size+START-256:(i+1)*batch_size+START+256]\n",
    "        \n",
    "        for j in range(batch_size):  # preprocessing original ecg sliding window\n",
    "            #print(j, end = '\\r')\n",
    "            \n",
    "            data = decomp(seq[j:j+512,:],'db2',(512,8) )\n",
    "            data_X[j][:4096] = data.reshape(4096)\n",
    "        \n",
    "        batch_end = time.time()\n",
    "        #print('time used for decompose: '+str(batch_end-start))\n",
    "        \n",
    "        model = load_model(model_path,custom_objects={'f1_value': f1_value})\n",
    "        prob[i*batch_size:(i+1)*batch_size] = model.predict(data_X).reshape(1, batch_size)[0] \n",
    "        del model # delete model after every batch prediction to avoid memory leak\n",
    "        gc.collect()\n",
    "        \n",
    "        end = time.time()\n",
    "        #print('Time used for predict '+ str(i) +': ' + str(end-batch_end))  \n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Filter and Binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(med_prob, threshold, start, end):\n",
    "    binarize_prob = np.empty((end-start),'int') # apply threshold = 0.5 to the probability\n",
    "    for i,prob in enumerate(med_prob[start:end]):\n",
    "        if prob<threshold:\n",
    "            binarize_prob[i] = 0\n",
    "        else:\n",
    "            binarize_prob[i] = 1\n",
    "    return binarize_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(start, prob, num, direction): \n",
    "    count,i = 0,start\n",
    "    while i < len(prob) and i >= 0 and prob[i]==num:\n",
    "        count += 1\n",
    "        i = i+1 if direction == 1 else i-1\n",
    "    return count\n",
    "\n",
    "\n",
    "'''Filter out predicted QRS periods with width <100ms'''\n",
    "\n",
    "def filter_by_width(binarize_prob):    \n",
    "    i = 0\n",
    "    while i<LEN: \n",
    "        if binarize_prob[i]==1:\n",
    "            count = counter(i,binarize_prob,1,1)\n",
    "            if count<=WIDTH_LOW: \n",
    "                for j in range(count):\n",
    "                    binarize_prob[i+j]=0\n",
    "            i += count\n",
    "            continue\n",
    "        i += 1\n",
    "    return binarize_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For two too closed QRS periods(<0.2s), discard the narrower one'''\n",
    "\n",
    "\n",
    "'''remove a disgnated period'''\n",
    "def remove(start, end, num, prob):  \n",
    "    for i in range(start,end):\n",
    "        prob[i] = num\n",
    "    return prob\n",
    "\n",
    "'''remove narrower period '''\n",
    "def remove_smaller(start, gap, prob): \n",
    "    prev_wid = counter(start-1, prob, 1, 0)    # get width\n",
    "    last_wid = counter(start+gap, prob, 1, 1) \n",
    "    if prev_wid < last_wid:                    # remove the previous QRS\n",
    "        prob = remove(start-prev_wid, start, 0, prob)\n",
    "        remove_prev = True\n",
    "    elif prev_wid >= last_wid:                 # remove the later QRS\n",
    "        prob = remove(start+gap, start+gap+last_wid, 0, prob)\n",
    "        remove_prev = False\n",
    "    return prob, remove_prev\n",
    "\n",
    "''' repeatedly check the next period and discard narrower period until the two adjacent periods are not too closed'''\n",
    "def remove_dis_low(k, gap, binarize_prob): \n",
    "    remove_prev = False\n",
    "    while gap<DIS_LOW and not remove_prev:\n",
    "        binarize_prob,remove_prev = remove_smaller(k, gap, binarize_prob)\n",
    "        gap = counter(k,binarize_prob,0,1)\n",
    "    return binarize_prob, gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For two too distant QRS periods(>1,5s), re-search the gap for missing QRS using lower threshold = 0.4'''\n",
    "\n",
    "def search_new(start, gap, med_prob,binarize_prob): # re-search gap for missing QRS with threshold = 0.4\n",
    "    end = start+ gap\n",
    "    prob_segment= binarize(med_prob, THRESHOLD2, start, end)\n",
    "    new_gap = counter(0,prob_segment,0,1)\n",
    "    if new_gap == 0:\n",
    "        wid = counter(0,prob_segment,1,1)\n",
    "        prob_segment = remove(0,wid,0,prob_segment)\n",
    "        new_gap = counter(0,prob_segment,0,1)\n",
    "    binarize_prob[start:end] = prob_segment\n",
    "    binarize_prob, gap = remove_dis_low(start,new_gap,binarize_prob) \n",
    "    return binarize_prob, gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''refine process main'''\n",
    "\n",
    "def refine(binarize_prob, med_prob):\n",
    "    k, meet_R = 0, False\n",
    "    while k < len(binarize_prob):\n",
    "        #print(k,end='\\r')\n",
    "        if binarize_prob[k] ==1 and not meet_R:\n",
    "            meet_R = True\n",
    "        elif meet_R and binarize_prob[k]==0:\n",
    "            gap = counter(k,binarize_prob,0,1)\n",
    "            if gap < DIS_LOW and k+gap<len(binarize_prob): # too closed\n",
    "                binarize_prob, gap = remove_dis_low(k, gap, binarize_prob) \n",
    "            if gap > DIS_HIGH: # too distant\n",
    "                binarize_prob, gap = search_new(k, gap, med_prob, binarize_prob)\n",
    "            k += gap\n",
    "            continue\n",
    "        k += 1\n",
    "    return binarize_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get location of predicted R'''\n",
    "def locate(binarize_prob,ecg):\n",
    "    Rs = np.zeros((len(binarize_prob)),'int')\n",
    "    count, j  = 0,0\n",
    "    while j < len(binarize_prob):\n",
    "        if binarize_prob[j] == 1:\n",
    "            count = counter(j,binarize_prob,1,1)\n",
    "            period = ecg[START:END][j:j+count]\n",
    "            index = np.argmax(np.abs(period))\n",
    "            Rs[j+index] = 1\n",
    "            j+=count\n",
    "            continue\n",
    "        j+=1\n",
    "    return Rs\n",
    "\n",
    "'''process true annotation to get true R position'''\n",
    "def get_ann(binarize_prob):\n",
    "    with open('../../record/20010/ann.txt','r') as f:\n",
    "        annot = np.zeros((len(binarize_prob)),'int')\n",
    "        for line in f:\n",
    "            pos, ann = line.rstrip().split(',')[:2]\n",
    "            pos = round(int(pos)/1000*256)\n",
    "            if ann =='X' or pos<START:\n",
    "                continue\n",
    "            elif pos>END:\n",
    "                break\n",
    "            else:\n",
    "                annot[pos-START] = 1\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''compare true R position and predicted R position; calculate recall and precision'''\n",
    "\n",
    "def correspond(ary, index):\n",
    "    for i in range(ERROR):\n",
    "        if ary[index+i]==1:\n",
    "            return index+i+1\n",
    "    return -1\n",
    "\n",
    "def measure(Rs, annot):\n",
    "    \n",
    "    nn,no,on, i = 0,0,0,0 #TP, FN, FP\n",
    "    \n",
    "    while i<len(annot):\n",
    "        if Rs[i] == 1:\n",
    "            index = correspond(annot, i)\n",
    "            if index == -1:\n",
    "                on += 1\n",
    "            else:\n",
    "                nn += 1\n",
    "                i = index\n",
    "                continue\n",
    "        elif annot[i] == 1:\n",
    "            index = correspond(Rs, i)\n",
    "            if index == -1:\n",
    "                no += 1\n",
    "            else:\n",
    "                nn += 1\n",
    "                i = index\n",
    "                continue\n",
    "        i+=1\n",
    "    se = nn/(nn+no+K.epsilon())\n",
    "    prec = nn/(nn+on+K.epsilon())\n",
    "    return se, prec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 256\n",
    "END = len(ecg)-256\n",
    "LEN = END-START\n",
    "RECORD = 20009\n",
    "BATCH_NUM = 128\n",
    "THRESHOLD1 = 0.5 #第一次binarize的阈值\n",
    "THRESHOLD2 = 0.4 #第二次搜索的阈值\n",
    "DIS_LOW = 52 #0.2s 可允许的两段1区间的最小距离\n",
    "DIS_HIGH = 307 #1.5s 可允许的两段1区间最大距离\n",
    "WIDTH_LOW = 24 #100ms 可允许的1区间最窄宽度\n",
    "ERROR = 8 #可偏差点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ecg_path ='../../record/20010/ecg.dat', model_path = '../../model/model_30.h5'):\n",
    "    ecg = np.fromfile(ecg_path, '>i2') # plot original ecg sequence\n",
    "    ecg = ecg[:23903744]\n",
    "    \n",
    "    prob = get_prob(ecg, model_path)\n",
    "    #print('finish predict')\n",
    "    med_prob = scipy.signal.medfilt(prob) # median filter to the probability\n",
    "    #print('finish median filter')\n",
    "    binarize_prob = binarize(med_prob,THRESHOLD1,0,LEN)\n",
    "    #print('finish binarize')\n",
    "    binarize_prob = filter_by_width(binarize_prob)\n",
    "    #print('finish filter by width')\n",
    "    binarize_prob = refine(binarize_prob, med_prob)\n",
    "    binarize_prob = filter_by_width(binarize_prob)\n",
    "    #print('finish refine')\n",
    "\n",
    "    Rs = locate(binarize_prob,ecg)\n",
    "    #print('finish locate R')\n",
    "    annot = get_ann(binarize_prob)\n",
    "    #print('finish get annot')\n",
    "    se, prec = measure(Rs, annot)\n",
    "    print('sensitivity: '+str(se)+ '   precision: '+ str(prec)) # model 30 error = 8\n",
    "    \n",
    "    return se, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "time used for decompose: 36.89652705192566\n",
      "Time used for predict 0: 15.880534648895264\n",
      "Batch 1\n",
      "time used for decompose: 35.361093521118164\n",
      "Time used for predict 1: 12.282869815826416\n",
      "Batch 2\n",
      "time used for decompose: 35.56828761100769\n",
      "Time used for predict 2: 12.082835674285889\n",
      "Batch 3\n",
      "time used for decompose: 35.56056356430054\n",
      "Time used for predict 3: 12.053177833557129\n",
      "Batch 4\n",
      "time used for decompose: 35.19124889373779\n",
      "Time used for predict 4: 12.070293664932251\n",
      "Batch 5\n",
      "time used for decompose: 35.69074869155884\n",
      "Time used for predict 5: 12.030882120132446\n",
      "Batch 6\n",
      "time used for decompose: 35.46727013587952\n",
      "Time used for predict 6: 12.065332174301147\n",
      "Batch 7\n",
      "time used for decompose: 35.295260190963745\n",
      "Time used for predict 7: 12.092155933380127\n",
      "Batch 8\n",
      "time used for decompose: 35.34883236885071\n",
      "Time used for predict 8: 12.07216501235962\n",
      "Batch 9\n",
      "time used for decompose: 36.00576210021973\n",
      "Time used for predict 9: 12.148287773132324\n",
      "Batch 10\n",
      "time used for decompose: 35.62011456489563\n",
      "Time used for predict 10: 12.08329176902771\n",
      "Batch 11\n",
      "time used for decompose: 35.223729610443115\n",
      "Time used for predict 11: 12.081787824630737\n",
      "Batch 12\n",
      "time used for decompose: 35.638389587402344\n",
      "Time used for predict 12: 12.076451301574707\n",
      "Batch 13\n",
      "time used for decompose: 36.160475730895996\n",
      "Time used for predict 13: 12.156226873397827\n",
      "Batch 14\n",
      "time used for decompose: 35.44769215583801\n",
      "Time used for predict 14: 12.078088760375977\n",
      "Batch 15\n",
      "time used for decompose: 35.3880455493927\n",
      "Time used for predict 15: 12.09380841255188\n",
      "Batch 16\n",
      "time used for decompose: 35.47074031829834\n",
      "Time used for predict 16: 12.189898014068604\n",
      "Batch 17\n",
      "time used for decompose: 35.74359726905823\n",
      "Time used for predict 17: 12.16334581375122\n",
      "Batch 18\n",
      "time used for decompose: 35.843149185180664\n",
      "Time used for predict 18: 12.205841779708862\n",
      "Batch 19\n",
      "time used for decompose: 35.20409107208252\n",
      "Time used for predict 19: 12.192152261734009\n",
      "Batch 20\n",
      "time used for decompose: 35.357396602630615\n",
      "Time used for predict 20: 12.0668466091156\n",
      "Batch 21\n",
      "time used for decompose: 35.170857667922974\n",
      "Time used for predict 21: 12.056539058685303\n",
      "Batch 22\n",
      "time used for decompose: 35.848334074020386\n",
      "Time used for predict 22: 12.017653942108154\n",
      "Batch 23\n",
      "time used for decompose: 35.438477993011475\n",
      "Time used for predict 23: 12.055911540985107\n",
      "Batch 24\n",
      "time used for decompose: 35.73684334754944\n",
      "Time used for predict 24: 12.024645805358887\n",
      "Batch 25\n",
      "time used for decompose: 35.05058026313782\n",
      "Time used for predict 25: 12.041921377182007\n",
      "Batch 26\n",
      "time used for decompose: 35.550806283950806\n",
      "Time used for predict 26: 12.065320014953613\n",
      "Batch 27\n",
      "time used for decompose: 35.48035025596619\n",
      "Time used for predict 27: 12.058024406433105\n",
      "Batch 28\n",
      "time used for decompose: 35.6371533870697\n",
      "Time used for predict 28: 12.095844984054565\n",
      "Batch 29\n",
      "time used for decompose: 35.31193470954895\n",
      "Time used for predict 29: 12.115659952163696\n",
      "Batch 30\n",
      "time used for decompose: 35.27851963043213\n",
      "Time used for predict 30: 12.248640537261963\n",
      "Batch 31\n",
      "time used for decompose: 35.598668813705444\n",
      "Time used for predict 31: 12.267648935317993\n",
      "Batch 32\n",
      "time used for decompose: 35.572492361068726\n",
      "Time used for predict 32: 12.141999006271362\n",
      "Batch 33\n",
      "time used for decompose: 35.378666400909424\n",
      "Time used for predict 33: 12.274547815322876\n",
      "Batch 34\n",
      "time used for decompose: 35.47862672805786\n",
      "Time used for predict 34: 12.262393712997437\n",
      "Batch 35\n",
      "time used for decompose: 35.23480796813965\n",
      "Time used for predict 35: 12.424102306365967\n",
      "Batch 36\n",
      "time used for decompose: 35.26460528373718\n",
      "Time used for predict 36: 12.196149110794067\n",
      "Batch 37\n",
      "time used for decompose: 35.27066493034363\n",
      "Time used for predict 37: 12.175453186035156\n",
      "Batch 38\n",
      "time used for decompose: 35.55714392662048\n",
      "Time used for predict 38: 12.155821323394775\n",
      "Batch 39\n",
      "time used for decompose: 35.525349378585815\n",
      "Time used for predict 39: 12.277894020080566\n",
      "Batch 40\n",
      "time used for decompose: 35.61054348945618\n",
      "Time used for predict 40: 12.257797956466675\n",
      "Batch 41\n",
      "time used for decompose: 35.09565544128418\n",
      "Time used for predict 41: 12.145002365112305\n",
      "Batch 42\n",
      "time used for decompose: 35.15427303314209\n",
      "Time used for predict 42: 12.10860013961792\n",
      "Batch 43\n",
      "time used for decompose: 35.51967239379883\n",
      "Time used for predict 43: 12.149896621704102\n",
      "Batch 44\n",
      "time used for decompose: 35.264631032943726\n",
      "Time used for predict 44: 12.206945419311523\n",
      "Batch 45\n",
      "time used for decompose: 35.34754037857056\n",
      "Time used for predict 45: 12.214999437332153\n",
      "Batch 46\n",
      "time used for decompose: 35.55025577545166\n",
      "Time used for predict 46: 12.109188556671143\n",
      "Batch 47\n",
      "time used for decompose: 35.45823097229004\n",
      "Time used for predict 47: 12.178164720535278\n",
      "Batch 48\n",
      "time used for decompose: 35.55981254577637\n",
      "Time used for predict 48: 12.148565769195557\n",
      "Batch 49\n",
      "time used for decompose: 35.45610690116882\n",
      "Time used for predict 49: 12.148336410522461\n",
      "Batch 50\n",
      "time used for decompose: 35.436084508895874\n",
      "Time used for predict 50: 12.224167108535767\n",
      "Batch 51\n",
      "time used for decompose: 35.72634172439575\n",
      "Time used for predict 51: 12.309068441390991\n",
      "Batch 52\n",
      "time used for decompose: 35.4461727142334\n",
      "Time used for predict 52: 12.20985722541809\n",
      "Batch 53\n",
      "time used for decompose: 35.4415819644928\n",
      "Time used for predict 53: 12.206339359283447\n",
      "Batch 54\n",
      "time used for decompose: 35.3946487903595\n",
      "Time used for predict 54: 12.469939231872559\n",
      "Batch 55\n",
      "time used for decompose: 35.67758774757385\n",
      "Time used for predict 55: 12.325617551803589\n",
      "Batch 56\n",
      "time used for decompose: 35.28778123855591\n",
      "Time used for predict 56: 12.214347124099731\n",
      "Batch 57\n",
      "time used for decompose: 35.50822925567627\n",
      "Time used for predict 57: 12.22976541519165\n",
      "Batch 58\n",
      "time used for decompose: 35.91312026977539\n",
      "Time used for predict 58: 12.354641675949097\n",
      "Batch 59\n",
      "time used for decompose: 35.0174195766449\n",
      "Time used for predict 59: 12.282158851623535\n",
      "Batch 60\n",
      "time used for decompose: 35.41756772994995\n",
      "Time used for predict 60: 12.367857217788696\n",
      "Batch 61\n",
      "time used for decompose: 35.21029806137085\n",
      "Time used for predict 61: 12.244569540023804\n",
      "Batch 62\n",
      "time used for decompose: 35.74337601661682\n",
      "Time used for predict 62: 12.244210243225098\n",
      "Batch 63\n",
      "time used for decompose: 35.2992627620697\n",
      "Time used for predict 63: 12.219033002853394\n",
      "Batch 64\n",
      "time used for decompose: 35.396570444107056\n",
      "Time used for predict 64: 12.208042621612549\n",
      "Batch 65\n",
      "time used for decompose: 35.457101821899414\n",
      "Time used for predict 65: 12.256916046142578\n",
      "Batch 66\n",
      "time used for decompose: 35.72025728225708\n",
      "Time used for predict 66: 12.268572568893433\n",
      "Batch 67\n",
      "time used for decompose: 35.09105849266052\n",
      "Time used for predict 67: 12.157224655151367\n",
      "Batch 68\n",
      "time used for decompose: 35.227822065353394\n",
      "Time used for predict 68: 12.267907857894897\n",
      "Batch 69\n",
      "time used for decompose: 35.54648399353027\n",
      "Time used for predict 69: 12.469880104064941\n",
      "Batch 70\n",
      "time used for decompose: 35.482728719711304\n",
      "Time used for predict 70: 12.228943347930908\n",
      "Batch 71\n",
      "time used for decompose: 35.48495435714722\n",
      "Time used for predict 71: 12.22244381904602\n",
      "Batch 72\n",
      "time used for decompose: 35.443784952163696\n",
      "Time used for predict 72: 12.200002670288086\n",
      "Batch 73\n",
      "time used for decompose: 36.035874128341675\n",
      "Time used for predict 73: 12.25958251953125\n",
      "Batch 74\n",
      "time used for decompose: 35.42793941497803\n",
      "Time used for predict 74: 12.361159563064575\n",
      "Batch 75\n",
      "time used for decompose: 35.35428833961487\n",
      "Time used for predict 75: 12.360782861709595\n",
      "Batch 76\n",
      "time used for decompose: 35.682122468948364\n",
      "Time used for predict 76: 12.282638311386108\n",
      "Batch 77\n",
      "time used for decompose: 35.670689821243286\n",
      "Time used for predict 77: 12.381772756576538\n",
      "Batch 78\n",
      "time used for decompose: 35.336299657821655\n",
      "Time used for predict 78: 12.28033447265625\n",
      "Batch 79\n",
      "time used for decompose: 35.64532947540283\n",
      "Time used for predict 79: 12.292822122573853\n",
      "Batch 80\n",
      "time used for decompose: 35.566768646240234\n",
      "Time used for predict 80: 12.29669976234436\n",
      "Batch 81\n",
      "time used for decompose: 35.99725127220154\n",
      "Time used for predict 81: 12.316278457641602\n",
      "Batch 82\n",
      "time used for decompose: 35.54140830039978\n",
      "Time used for predict 82: 12.31895637512207\n",
      "Batch 83\n",
      "time used for decompose: 35.53878998756409\n",
      "Time used for predict 83: 12.407209873199463\n",
      "Batch 84\n",
      "time used for decompose: 35.40901064872742\n",
      "Time used for predict 84: 12.641185760498047\n",
      "Batch 85\n",
      "time used for decompose: 35.79257106781006\n",
      "Time used for predict 85: 12.3936767578125\n",
      "Batch 86\n",
      "time used for decompose: 35.560659646987915\n",
      "Time used for predict 86: 12.331691980361938\n",
      "Batch 87\n",
      "time used for decompose: 35.51832938194275\n",
      "Time used for predict 87: 12.210250616073608\n",
      "Batch 88\n",
      "time used for decompose: 35.384044885635376\n",
      "Time used for predict 88: 12.30836820602417\n",
      "Batch 89\n",
      "time used for decompose: 35.228742599487305\n",
      "Time used for predict 89: 12.317980527877808\n",
      "Batch 90\n",
      "time used for decompose: 35.13829302787781\n",
      "Time used for predict 90: 12.300087928771973\n",
      "Batch 91\n",
      "time used for decompose: 35.32674717903137\n",
      "Time used for predict 91: 12.296242475509644\n",
      "Batch 92\n",
      "time used for decompose: 35.842655420303345\n",
      "Time used for predict 92: 12.2438485622406\n",
      "Batch 93\n",
      "time used for decompose: 35.53769636154175\n",
      "Time used for predict 93: 12.269363403320312\n",
      "Batch 94\n",
      "time used for decompose: 35.49151659011841\n",
      "Time used for predict 94: 12.326837539672852\n",
      "Batch 95\n",
      "time used for decompose: 35.178558349609375\n",
      "Time used for predict 95: 12.418243646621704\n",
      "Batch 96\n",
      "time used for decompose: 35.91742420196533\n",
      "Time used for predict 96: 12.342716455459595\n",
      "Batch 97\n",
      "time used for decompose: 35.37876796722412\n",
      "Time used for predict 97: 12.361227035522461\n",
      "Batch 98\n",
      "time used for decompose: 35.99714922904968\n",
      "Time used for predict 98: 12.461835145950317\n",
      "Batch 99\n",
      "time used for decompose: 35.453152656555176\n",
      "Time used for predict 99: 12.427011728286743\n",
      "Batch 100\n",
      "time used for decompose: 35.51732015609741\n",
      "Time used for predict 100: 12.397732734680176\n",
      "Batch 101\n",
      "time used for decompose: 35.14291548728943\n",
      "Time used for predict 101: 12.367015838623047\n",
      "Batch 102\n",
      "time used for decompose: 35.27750325202942\n",
      "Time used for predict 102: 12.488851070404053\n",
      "Batch 103\n",
      "time used for decompose: 35.30578112602234\n",
      "Time used for predict 103: 12.488637447357178\n",
      "Batch 104\n",
      "time used for decompose: 35.26364827156067\n",
      "Time used for predict 104: 12.398613452911377\n",
      "Batch 105\n",
      "time used for decompose: 35.343733072280884\n",
      "Time used for predict 105: 12.484892845153809\n",
      "Batch 106\n",
      "time used for decompose: 35.75699329376221\n",
      "Time used for predict 106: 12.401265382766724\n",
      "Batch 107\n",
      "time used for decompose: 35.896594285964966\n",
      "Time used for predict 107: 12.460465669631958\n",
      "Batch 108\n",
      "time used for decompose: 35.58504867553711\n",
      "Time used for predict 108: 12.355279445648193\n",
      "Batch 109\n",
      "time used for decompose: 35.77700734138489\n",
      "Time used for predict 109: 12.392274141311646\n",
      "Batch 110\n",
      "time used for decompose: 35.74029350280762\n",
      "Time used for predict 110: 12.50374460220337\n",
      "Batch 111\n",
      "time used for decompose: 35.777456521987915\n",
      "Time used for predict 111: 12.392393350601196\n",
      "Batch 112\n",
      "time used for decompose: 35.36679530143738\n",
      "Time used for predict 112: 12.401530027389526\n",
      "Batch 113\n",
      "time used for decompose: 35.51467943191528\n",
      "Time used for predict 113: 12.400426864624023\n",
      "Batch 114\n",
      "time used for decompose: 35.28631615638733\n",
      "Time used for predict 114: 12.413110971450806\n",
      "Batch 115\n",
      "time used for decompose: 36.228532552719116\n",
      "Time used for predict 115: 12.353728532791138\n",
      "Batch 116\n",
      "time used for decompose: 35.412801027297974\n",
      "Time used for predict 116: 12.389967679977417\n",
      "Batch 117\n",
      "time used for decompose: 35.09080672264099\n",
      "Time used for predict 117: 12.396263599395752\n",
      "Batch 118\n",
      "time used for decompose: 35.318392753601074\n",
      "Time used for predict 118: 12.446328163146973\n",
      "Batch 119\n",
      "time used for decompose: 35.27194595336914\n",
      "Time used for predict 119: 12.423795700073242\n",
      "Batch 120\n",
      "time used for decompose: 35.68379306793213\n",
      "Time used for predict 120: 12.491264581680298\n",
      "Batch 121\n",
      "time used for decompose: 35.21290946006775\n",
      "Time used for predict 121: 12.465509414672852\n",
      "Batch 122\n",
      "time used for decompose: 35.32244253158569\n",
      "Time used for predict 122: 12.51542353630066\n",
      "Batch 123\n",
      "time used for decompose: 35.39380693435669\n",
      "Time used for predict 123: 12.537134170532227\n",
      "Batch 124\n",
      "time used for decompose: 35.49551558494568\n",
      "Time used for predict 124: 12.541721820831299\n",
      "Batch 125\n",
      "time used for decompose: 35.43418002128601\n",
      "Time used for predict 125: 12.43601107597351\n",
      "Batch 126\n",
      "time used for decompose: 36.08206343650818\n",
      "Time used for predict 126: 12.427595853805542\n",
      "Batch 127\n",
      "time used for decompose: 35.610658407211304\n",
      "Time used for predict 127: 12.452042579650879\n",
      "(21995520,)\n",
      "finish predict\n",
      "finish median filter\n",
      "finish binarize\n",
      "finish filter by width\n",
      "finish refine\n",
      "finish locate R\n",
      "finish get annot\n",
      "sensitivity: 0.990342144847716   precision: 0.9922720989161813\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot after Each Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = get_prob(ecg, model)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) # plot the original ecg along with the probability\n",
    "plt.plot(prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_prob = scipy.signal.medfilt(prob)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) # plot the original ecg along with the probability\n",
    "plt.plot(med_prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize_prob = binarize(med_prob,THRESHOLD1,0,LEN)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) # plot the original ecg along with the probability\n",
    "plt.plot(binarize_prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize_prob = filter_by_width(binarize_prob)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) # plot the original ecg along with the probability\n",
    "plt.plot(binarize_prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize_prob = refine(binarize_prob,med_prob)\n",
    "binarize_prob = filter_by_width(binarize_prob)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) # plot the original ecg along with the probability\n",
    "plt.plot(binarize_prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = locate(binarize_prob,ecg)\n",
    "annot = get_ann(binarize_prob)\n",
    "\n",
    "figure = plt.figure(figsize = (10,5)) \n",
    "plt.plot(binarize_prob,color = \"orange\")\n",
    "plt.plot(ecg[START:END]/2048, color = \"blue\")\n",
    "\n",
    "for i,R in enumerate(Rs):\n",
    "    if R ==1:\n",
    "        plt.axvline(i, color = 'red')\n",
    "        \n",
    "for i,ann in enumerate(annot):\n",
    "    if ann==1:\n",
    "        plt.axvline(i, color = 'green')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
